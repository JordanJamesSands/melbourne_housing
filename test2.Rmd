---
title: "test2"
author: "Jordan Sands"
date: "18 April 2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE , warning=FALSE)
```

```{r timer-1, include=FALSE}
START = Sys.time()
```

## Synopsis

Data related to Melbourne's property market have been attained online via
domain.com.au. The goal of this analysis is to better understand the property 
market, generate a predictive model, and
compare accuracy of machine learning methods. OpenStreetMap was used to import
additional data to the dataset before an array of machine learning models were
fitted
and ensembled using stacking. It has been found that gradient boosted 
decision trees perform best on this task although a final conclusion is yet
to be published.

## Introduction

This analysis concerns Melbourne's property market, a topic of much debate in today's social and political climate. The focus of this analysis is to better understand the contributing factors to property value, change in value over time is not addressed here.

The primary objectives of this analysis are threefold:

* **Prediction:** It would be useful to be able to predict the sale price of a
property.

* **Description:** Understanding what makes a valuable property in Melbourne is
useful knowledge for any home owner, it also makes for an interesting analysis.

* **Model Assessment:** We're going to fit a variety of models. Their accuracy
will be compared, along with any strengths and weaknesses specific to this dataset.

The `R` code for the analysis can be viewed by hitting the 'code' buttons along the way, alternatively you can show all code chunks by selecting the option in the code dropdown menu at the top right of the page.

## Data Preparation

The data have been downloaded from the online property website domain.com.au by
a member of the online data science community kaggle.com. The post can be viewed
[here](https://www.kaggle.com/anthonypino/melbourne-housing-market). Thank you
to Kaggle user anthonypino.

```{r load-deps_read-in, message=FALSE, warning=FALSE}
#-------------------load dependencies-------------------------------------------

#dataframe manipulation
require(plyr)       
require(dplyr)      
require(tidyr)

#modelling
require(caret)      
require(xgboost)    
require(gam)        

#Visualisation
require(ICEbox)
require(pdp)
require(plotly)     
require(RColorBrewer)
require(htmltools)
require(xtable)

#parallelisation
require(doParallel)
require(foreach)    

require(png)        #for loading images
require(osmdata)    #to import data from open street map
require(usedist)    #For computing distances
require(leaflet)    #interactive plotting of geospatial data
require(sf)         #handling geospatial data
require(geosphere)  #computing on geospatial data
require(rgdal)      #computing on geospatial data
require(knitr)      #markdown utilities

#other
require(Hmisc)
require(gsubfn)

#-------------------------read in data------------------------------------------
property_data <- read.csv('original_data/Melbourne_housing_FULL.csv')

#----------------------rename columns-------------------------------------------
names(property_data) <- c('suburb','add','nrooms','type','price','method','agent','date',
                          'precomputeddist','pc','nbedroom','nbathroom','ncar','land_area',
                          'building_area','year_built','council_area','lat','lng','region',
                          'propcount')

#-----------------------------recast types--------------------------------------
property_data$date = as.Date(property_data$date,format='%d/%m/%Y')
property_data$suburb = property_data$suburb %>% 
    as.character %>% tolower %>% capitalize %>% as.factor
property_data$add = as.character(property_data$add)
property_data$propcount = as.numeric(property_data$propcount)
property_data$precomputeddist = as.numeric(property_data$precomputeddist)

#------------------Additional Steps---------------------------------------------

#relabel the type variable
full_names <- c('h'='House','t'='Townhouse','u'='Unit')
property_data$type <- revalue(property_data$type,replace=full_names)

#create some log transforms for area variables, this gives a slight improvement
property_data$building_area_log <- log(property_data$building_area)
property_data$land_area_log <- log(property_data$land_area +1)


#give every object an ID
property_data$ID = as.character(1:nrow(property_data))

#keep only data with price available
property_data <- property_data[!is.na(property_data$price),]
```

```{r save_for_show-1 , include=F}
save(property_data,file='property_data1.Robject')
```

After parsing an `ID` column is appended to the data. The table has 
`r nrow(property_data)` records of sales, each with `r ncol(property_data)` 
features. 

[View a sample of the data here](show_data.html).

Looking ahead, these are the most important features:

`nrooms`: The number of rooms. this may be inaccurate, but is likely correlated
with the true number of rooms.

`type`: The type of the property, one of 'House', 'Townhouse' or 'Unit'.

`nbathroom`: number of bathrooms

`ncar`: number of car parks

`land_area`: Land area in square metres

`building_area`: building area in square metres

`year_built`: year built

`lat`: latitude ordinates of the property

`lng`: longitude ordinates of the property

The goal is to predict `log(price)` with respect to root mean square loss. A log
transform is sensible as we expect, (and observe), higher variance in sale price
for more expensive properties. The transform tames the 
heteroscedasticity in variance.

### Quality Check

First it must be established that this data from domain.com.au is accurate. To check this, an xlsx file of median house price by suburb is downloaded from Australia's Department of Environment, Land, Water and Planning. You can [download the data yourself here](https://www.propertyandlandtitles.vic.gov.au/__data/assets/excel_doc/0015/54213/suburb_house_2017.xls)

```{r median-comparison, echo=TRUE, message=FALSE, warning=FALSE}
#parse government data
#note that some minor edits were manually applied to remove headers before reading the document in to R
gov_median_data <- read.csv('other_data/suburb_house_2017_edited.csv',stringsAsFactors = F)
names(gov_median_data)[1] <- 'suburb'
gov_median_data$suburb <- gov_median_data$suburb %>% tolower %>% capitalize

#limit property_data to 2017
property_data_2017 <- subset(property_data,date<'2018-01-01' & date > '2016-12-31')
#limit to houses
property_data_2017 <- property_data_2017[property_data_2017$type=='House',]


#select relevant columns
gov_median_data <-  gov_median_data %>% select(c(suburb,X2017))
#rename column
names(gov_median_data)[2] <- 'median_price'
property_data_2017 <- group_by(property_data_2017,suburb) %>% 
    summarise(median_price = median(price,na.rm=T),count=n())

#record how many properties were sold in this area, compared to total
property_data_2017$prop <- property_data_2017$count/sum(property_data_2017$count)

comparison_data <- merge(property_data_2017,gov_median_data,by='suburb',suffixes = c('_pd','_gov'))


plot_ly(data=comparison_data,
        x=~median_price_gov,
        y=~median_price_pd,
        type='scatter',
        mode='markers',
        text=paste0(comparison_data$suburb,'\n',comparison_data$count,' sales'),
        hoverinfo='text'
) %>% add_lines(x=c(0,4e+6),y=c(0,4e+6),inherit = F) %>%
    layout(showlegend=F,
           xaxis=list('title'='Median Sale Price in this Dataset'),
           yaxis=list('title'='Median House Sale Price According to Aus. Government'))


```

It appears to be a good match, more analysis was performed to verify the integrity of the data, but details are spared.

#### NA values

While exploring the data it is necessary to investigate how complete it is.
Below is a heatmap of `NA` values in the dataset, rows represent features and
columns property sales.  The rows and columns have been clustered
to better represent the distribution of `NA` elements.

It should be noted that the figure below excludes
property sales without any `NA` values. This heatmap represents
`r paste0(round(mean(!complete.cases(property_data))*100,2),'%')` of the data.

```{r na-heatmap , include=TRUE, message=FALSE, warning=FALSE , fig.width=10 , fig.height=10}
#--This task was too computationally expensive to run inside a markdown doc-----
fig <- readPNG('na_heatmap.png')
plot.new()
rasterImage(fig,0,0,1,1)
```


The above figure shows significant clustering of `NA` values among observations
and features; many observations have missing values for the same features. This
is especially true for `lng`, `lat`, `nbathroom`, `nbedroom`, `ncar` and 
`land_area`. Many of these features, as well as `building_area` and `year_built`
will later prove useful in predictive 
modelling. Imputation was attempted, but with no significant success, 
and so a desicion was made to exclude values with `NA` entries in these fields
from the analysis.





### Exploration and Cleaning

#### Geospatial Data

A plot of the data, aggregated by suburb:

```{r cleaning-location ,eval=F, message=FALSE, warning=FALSE , fig.height=3}
#------------------------read suburb data---------------------------------------

#read in suburb boundaries
locs <-  readOGR('other_data/VIC_LOCALITY_POLYGON_shp.shp',
                 GDAL1_integer64_policy = TRUE,stringsAsFactors = F,verbose=F)

#Easter egg:
#There are some far away regional places with the same name as Melbourne suburbs

#locs$VIC_LOCA_2 has suburb names, parse them to be like analysis data
locs$VIC_LOCA_2 <- capitalize(tolower(locs$VIC_LOCA_2))


#--------------------aggregate data used in analysis----------------------------
suburb_data <- group_by(property_data,suburb) %>% summarise(
    median_price = median(price,na.rm=T),
    q25 = quantile(price,0.25,na.rm=T),
    q75 = quantile(price,0.75,na.rm=T),
    mean_lng = mean(lng,na.rm=T),
    mean_lat = mean(lat,na.rm=T),
    count0 = n(),
    count = sum(!is.na(price))
)

#---------------------------combine suburb and analysis data--------------------
#find suburbs in both datasets
locs_subset <- locs[which(locs$VIC_LOCA_2 %in% levels(suburb_data$suburb)),]

#merge data from suburb_data to locs_subset
merged <- merge(locs_subset,suburb_data,by.x='VIC_LOCA_2',by.y='suburb')

#----------------------------setup interactive text-----------------------------
prettify <- function(number) {
    format(round(number,0),big.mar=',',scientific = FALSE)
}
merged$string <- paste0(
    merged$VIC_LOCA_2,
    '<br>median price: $',prettify(merged$median_price),
    '<br>25th%: $',prettify(merged$q25),
    '<br>75th%: $',prettify(merged$q75),
    '<br>count: ',merged$count
)

#----------------------------setup colours--------------------------------------
merged$median_perc <- percent_rank(merged$median_price)
pal <- colorNumeric('Blues',domain=range(merged$median_perc,na.rm=T))

#--------------------------------plot-------------------------------------------
lngview <- mean(merged$mean_lng,na.rm=T)
latview <- mean(merged$mean_lat,na.rm=T)
leaflet(merged) %>% addTiles() %>% 
    setView(lat=latview,lng =lngview , zoom=9) %>% 
    addPolygons(fillColor = ~pal(median_perc),
                fillOpacity = 0.9,
                weight=1,
                label=~lapply(string,HTML)
    )

```

#### Univariate Exploration

Below, `log(price)` is plotted against a few select features in the dataset. 
These plots inform cleaning, see the code snippets for detailed information.

```{r not_sold}
#-------------not sold, (method)
#some methods detail properties that wern't actually sold
not_sold <- property_data$method %in% c('PI','NB','VB','W')
```

```{r cleaning-year_built , message=FALSE, warning=FALSE , fig.height=3}
#lng and lat have very few missing, dont wanna bother imputing
loc_bool <- !is.na(property_data$lng) & !(is.na(property_data$lat))

#----------------------year_built
#1 property was apparaently built in 1196 and another next century
to_correct <- which(property_data$year_built > 2030| property_data$year_built < 1788) 
property_data[to_correct,'year_built'] = NA
year_built_bool <- !is.na(property_data$year_built)
```

```{r cleaning-ncar , message=FALSE, warning=FALSE , fig.height=3}
#--------------------ncar
#ggplot(data= property_data, aes(x=ncar,y=log(price))) + 
  #  geom_jitter(alpha=0.4,color='#0078D7')
#limit the scope of analysis to properties with fewer than 7 car parks
ncar_bool <- !is.na(property_data$ncar) & property_data$ncar<=6
```

```{r cleaning-nbathroom , message=FALSE, warning=FALSE , fig.height=3}
#--------------------nbathroom
#ggplot(property_data,aes(x=nbathroom,y=log(price))) + 
  #  geom_jitter(alpha=0.4,color='#0078D7')
#limit the analysis to properties with nbathroom <= 4
bath_bool <- !is.na(property_data$nbathroom) & (property_data$nbathroom<=4 & property_data$nbathroom>0)
```

```{r cleaning-nrooms , message=FALSE, warning=FALSE , fig.height=3}
#---------------------nrooms
#ggplot(property_data,aes(x=nrooms,y=log(price))) + 
 #   geom_jitter(alpha=0.4,color='#0078D7')
#limit the analysis to properties with nrooms <=6
nroom_bool <- property_data$nrooms<=6 & property_data$nrooms>0
```


```{r cleaning-nbedroom , include=FALSE, message=FALSE, warning=FALSE , fig.height=3}
#---------------------nbedroom
has_bedroom <- !is.na(property_data$nbedroom)
cor_nbedroom_nrooms <- cor(property_data$nbedroom[has_bedroom],property_data$nrooms[has_bedroom])
#dont include in analysis
```

```{r building_area , message=FALSE, warning=FALSE , fig.height=3}
#----------------------building_area
#make ==0 NA
property_data[is.na(property_data$building_area) | property_data$building_area==0,'building_area'] = NA


#limit analysis to buildings with less than 1000 building_area
BA_bool <- !is.na(property_data$building_area) & property_data$building_area<1000
```

```{r cleaning-land_area , message=FALSE, warning=FALSE , fig.height=3}
#----------------------land_area
#limit analysis to properties with land_area < 10000
land_bool <- !is.na(property_data$land_area) & property_data$land_area < 10000
```

```{r include=T}
#To parse in to the report's markdown text
land_too_big <- property_data$land_area<10000
building_too_big <- property_data$building_area<1000
ndropped_because_too_big <- sum(land_too_big | building_too_big,na.rm=T)
```

```{r cleaning-actions , message=FALSE, warning=FALSE , fig.height=3}
#---------------------actions

#gather bools of which rows can be kept, intersection will be kept
property_data <- property_data[nroom_bool & bath_bool & land_bool & BA_bool & ncar_bool & loc_bool & year_built_bool & !not_sold,]

#replot
#ggplot(property_data,aes(x=log(building_area),y=log(price))) + 
#    geom_point(alpha=0.4,color='#0078D7')
#ggplot(property_data,aes(x=log(land_area+1),y=log(price))) + 
#    geom_point(alpha=0.4,color='#0078D7')

```

`r ndropped_because_too_big`